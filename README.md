
# Streamlit with Ollama Integration

![image](https://github.com/AndreRatzenberger/streamlit-ollama-chat/assets/44863088/4479bb83-e199-443f-9af7-7edd3ea1c8e0)


## Overview

This repository contains a simple boilerplate application that integrates Ollama into Streamlit to enable chat functionalities using models provided by Ollama.

The app features a sidebar that allows users to switch between different models provided by Ollama.


## Running the Application

To run this application, use the following command:

```bash
streamlit run main.py
```

This will start the Streamlit server locally and open the application in your default web browser.

## Requirements

This application requires the following Python packages:

- ollama==0.1.8 [Github](https://github.com/ollama/ollama-python) | [web](https://ollama.com/)
- streamlit==1.33.0 [Github](https://github.com/streamlit/streamlit) | [web](https://streamlit.io/)

You can install these packages using pip:

```bash
pip install ollama==0.1.8 streamlit==1.33.0
```
